{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Content Generating AI (Using Vader NLP) for Marketing Team "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:03.598463Z",
     "start_time": "2019-07-29T14:21:01.611601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import warnings\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import warnings\n",
    "import datetime as dt\n",
    "from datetime import timedelta\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "\n",
    "headers = {\"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done: Live Coin Watch, Coindesk, Forexlive, BBC, Cointelegraph,Techbullion, Venturebeat, Dailyfintech, Breakermag, Quartz, Coinspeaker, Cryptocompare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:03.667442Z",
     "start_time": "2019-07-29T14:21:03.603460Z"
    }
   },
   "outputs": [],
   "source": [
    "olddata=pd.read_json('dataset.json')\n",
    "oldlinks = olddata['Links']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Coin Watch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:07.521256Z",
     "start_time": "2019-07-29T14:21:03.674420Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "articles = [[],[],[],[],[],[]]\n",
    "\n",
    "url = \"https://news.livecoinwatch.com/\"\n",
    "\n",
    "response = requests.get(url, headers)\n",
    "soup = bs(response.text, \"lxml\")\n",
    "\n",
    "x = soup.find_all('h3')\n",
    "for rawlink in x:\n",
    "    link = rawlink.find('a').get('href')\n",
    "    if link not in set(oldlinks):\n",
    "        try:\n",
    "            articlesoup = bs(requests.get(link).text, 'html.parser')\n",
    "            #if timestamp>time:\n",
    "            articletext = articlesoup.article.find('div','td-post-content').get_text(' ', strip=True)\n",
    "            wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "            words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "            articles[0].append(articlesoup.find('title').get_text())\n",
    "            articles[1].append(pd.Timestamp(articlesoup.article.find('time').get_text()))\n",
    "            articles[2].append(link)\n",
    "            articles[3].append(articletext)\n",
    "            articles[4].append(wordslength)\n",
    "            articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "livecoinwatch = pd.DataFrame()\n",
    "\n",
    "livecoinwatch['Title'], livecoinwatch['Timestamp'], livecoinwatch['Links'], livecoinwatch['Text'], livecoinwatch['Wordslength']  = articles[0],pd.to_datetime(articles[1]),articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Forexlive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:08.314624Z",
     "start_time": "2019-07-29T14:21:07.528252Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = [[],[],[],[],[],[]]\n",
    "url = \"https://www.forexlive.com/Cryptocurrency\"\n",
    "response = requests.get(url, headers)\n",
    "soup = bs(response.text, 'lxml')\n",
    "x = soup.find_all('article')\n",
    "for rawlink in x:\n",
    "        link = 'https:' + rawlink.find('a').get('href')\n",
    "        if link not in set(oldlinks):\n",
    "            try:\n",
    "                articlesoup = bs(requests.get(link, headers=headers).text, 'html.parser')\n",
    "                articletext = articlesoup.article.find_all('div','artbody')[0].get_text(' ', strip=True)\n",
    "                words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "                wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "                articles[0].append(articlesoup.find('title').get_text())\n",
    "                articles[1].append(pd.to_datetime(articlesoup.article.find_all('time')[1].get_text()))\n",
    "                articles[2].append(link) \n",
    "                articles[3].append(articletext)\n",
    "                articles[4].append(wordslength)\n",
    "                articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    \n",
    "forexlive = pd.DataFrame()\n",
    "forexlive['Title'], forexlive['Timestamp'], forexlive['Links'], forexlive['Text'], forexlive['Wordslength']  = articles[0],pd.to_datetime(articles[1]),articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Coindesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:08.863838Z",
     "start_time": "2019-07-29T14:21:08.317584Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = [[],[],[],[],[],[]]\n",
    "url = \"https://www.coindesk.com/\"\n",
    "response = requests.get(url, headers)\n",
    "soup = bs(response.text, 'lxml')\n",
    "x=soup.find('div',id='body-container').find_all('a')\n",
    "for rawlink in x:\n",
    "    link = rawlink.get('href')\n",
    "    if link not in set(oldlinks):\n",
    "        try:\n",
    "            articlesoup = bs(requests.get(link).text, 'html.parser')\n",
    "            articletext = articlesoup.find_all('div','entry-content')[0].get_text(' ', strip=True)\n",
    "            wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "            words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "            articles[0].append(articlesoup.find('title').get_text())\n",
    "            articles[1].append(pd.to_datetime(articlesoup.article.find(\"div\",'timestamp').find(\"span\").get_text()))\n",
    "            articles[2].append(link)\n",
    "            articles[3].append(articletext)\n",
    "            articles[4].append(wordslength)\n",
    "            articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "\n",
    "coindesk = pd.DataFrame()\n",
    "coindesk['Title'], coindesk['Timestamp'], coindesk['Links'], coindesk['Text'], coindesk['Wordslength']  = articles[0],pd.to_datetime(articles[1]),articles[2],articles[3],articles[4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cointelegraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:10.543355Z",
     "start_time": "2019-07-29T14:21:08.868831Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import feedparser\n",
    "\n",
    "articles = [[],[],[],[],[],[]]\n",
    "url = \"https://cointelegraph.com/rss\"\n",
    "response = feedparser.parse(url)\n",
    "\n",
    "for rawlink in response.entries:\n",
    "    link = rawlink['link']\n",
    "    if link not in set(oldlinks):\n",
    "        try:\n",
    "            \n",
    "            articles[0].append(rawlink[\"title\"])\n",
    "            articles[1].append(pd.to_datetime(rawlink['published']))\n",
    "            articles[2].append(link)\n",
    "            articlesoup = bs(requests.get(link, headers=headers).text, 'html.parser')\n",
    "\n",
    "            p = articlesoup.find_all('div','post-full-text contents js-post-full-text')\n",
    "            if len(p) > 0:\n",
    "                articletext = p[0].get_text(' ', strip=True)\n",
    "                wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "                words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "                articles[3].append(articletext)\n",
    "                articles[4].append(wordslength)\n",
    "                articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "            else:\n",
    "                p=articlesoup.find('div','content_block__inner col-xs-12 col-sm-12 col-md-8 col-lg-8 pull-right')\n",
    "                articletext = p.get_text(' ', strip=True)\n",
    "                wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "                words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "                articles[3].append(articletext)\n",
    "                articles[4].append(wordslength)\n",
    "                articles[5].append(words) \n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        \n",
    "cointelegraph = pd.DataFrame()\n",
    "cointelegraph['Title'], cointelegraph['Timestamp'], cointelegraph['Links'], cointelegraph['Text'], cointelegraph['Wordslength']  = articles[0],pd.to_datetime(articles[1]),articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not pulled from: Coinstats, CoinGecko, Messari, NewsAPI, Coinlore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crypto compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:11.109237Z",
     "start_time": "2019-07-29T14:21:10.551352Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.request import *\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "coinlisthttps = urlopen(\"https://min-api.cryptocompare.com/data/v2/news/?lang=EN\").read().decode('utf-8')\n",
    "coinlistdata = json.loads(coinlisthttps)[\"Data\"]\n",
    "\n",
    "articles = [[],[],[],[],[],[]]\n",
    "for rawlink in coinlistdata:    \n",
    "    link = rawlink[\"url\"]\n",
    "    if link not in set(oldlinks):\n",
    "        articles[0].append(rawlink[\"title\"])\n",
    "        articles[1].append(datetime.fromtimestamp(rawlink[\"published_on\"]).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        articles[2].append(link)\n",
    "        articles[3].append(rawlink[\"body\"])\n",
    "        wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  rawlink[\"body\"]).split())\n",
    "        words = re.sub('[^a-z\\ \\']+', ' ',  rawlink[\"body\"]).split()\n",
    "        articles[4].append(wordslength)\n",
    "        articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "\n",
    "        \n",
    "cryptocompare = pd.DataFrame()\n",
    "cryptocompare['Title'], cryptocompare['Timestamp'], cryptocompare['Links'], cryptocompare['Text'], cryptocompare['Wordslength']  = articles[0],articles[1],articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:16.193384Z",
     "start_time": "2019-07-29T14:21:11.116223Z"
    }
   },
   "outputs": [],
   "source": [
    "res = requests.get('https://www.bbc.co.uk/search?q=cryptocurrency',headers=headers)\n",
    "soup = bs(res.content,'lxml')\n",
    "\n",
    "articles = [[],[],[],[],[],[]]\n",
    "x = soup.find(class_='search-results results').find_all('li')\n",
    "for rawlink in x:\n",
    "    link = rawlink.find('a').get('href')\n",
    "    if link not in set(oldlinks):\n",
    "        try:\n",
    "            articlesoup = bs(requests.get(link).text, 'html.parser')\n",
    "            articletext = articlesoup.find('div',class_='story-body__inner')\n",
    "            articletext.figure.decompose()\n",
    "            articletext.div.decompose()\n",
    "            articletext=articletext.get_text(' ', strip=True)\n",
    "            wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "            words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "            articles[0].append(articlesoup.find(class_='story-body__h1').get_text())\n",
    "            articles[1].append(pd.to_datetime(articlesoup.find('div',class_='date date--v2').get('data-datetime')))\n",
    "            articles[2].append(link)\n",
    "            articles[3].append(articletext)\n",
    "            articles[4].append(wordslength)\n",
    "            articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "        except:\n",
    "            pass\n",
    "bbc = pd.DataFrame()\n",
    "bbc['Title'], bbc['Timestamp'], bbc['Links'], bbc['Text'], bbc['Wordslength']  = articles[0],articles[1],articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coinspeaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:17.915791Z",
     "start_time": "2019-07-29T14:21:16.196382Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = requests.get('https://www.coinspeaker.com/',headers=headers)\n",
    "soup = bs(res.content,'lxml')\n",
    "articles = [[],[],[],[],[],[]]\n",
    "\n",
    "x = soup.find('div',id='content').find_all('h3')\n",
    "for rawlink in x:\n",
    "    link = rawlink.find('a').get('href')\n",
    "    if link not in set(oldlinks):\n",
    "        try:\n",
    "            articlesoup = bs(requests.get(link).text, 'html.parser')\n",
    "            articletext = articlesoup.find('div',class_='entry-content').get_text('',strip=True)\n",
    "            wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "            words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "            articles[0].append(articlesoup.find('h1',class_='entry-title-spotlight').get_text())\n",
    "            articles[1].append(pd.to_datetime(articlesoup.find(class_='timestamp').get_text().replace('Published on ','').replace(' Updated on','')))\n",
    "            articles[2].append(link)\n",
    "            articles[3].append(articletext)\n",
    "            articles[4].append(wordslength)\n",
    "            articles[5].append(words)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "Coinspeaker = pd.DataFrame()\n",
    "Coinspeaker['Title'], Coinspeaker['Timestamp'], Coinspeaker['Links'], Coinspeaker['Text'], Coinspeaker['Wordslength']  = articles[0],articles[1],articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:17.939739Z",
     "start_time": "2019-07-29T14:21:17.919749Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Links</th>\n",
       "      <th>Text</th>\n",
       "      <th>Wordslength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, Timestamp, Links, Text, Wordslength]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Coinspeaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Techbullion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:21.957100Z",
     "start_time": "2019-07-29T14:21:17.945733Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = requests.get('https://www.techbullion.com/?s=cryptocurrency')\n",
    "soup = bs(res.content,'lxml')\n",
    "articles = [[],[],[],[],[],[]]\n",
    "x = soup.find(class_='archive-col-list left relative infinite-content').find_all('li')\n",
    "for rawlink in x:\n",
    "    link = rawlink.find('a').get('href')\n",
    "    if link not in set(oldlinks):\n",
    "        try:\n",
    "            articlesoup = bs(requests.get(link).text, 'html.parser')\n",
    "            articletext = articlesoup.find(id='content-main').get_text('',strip=True)\n",
    "            wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "            words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "            articles[0].append(articlesoup.find('header').find('h1').get_text())\n",
    "            articles[1].append(pd.to_datetime(articlesoup.find('time').get('datetime')))\n",
    "            articles[2].append(link)\n",
    "            articles[3].append(articletext)\n",
    "            articles[4].append(wordslength)\n",
    "            articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "Techbullion = pd.DataFrame()\n",
    "Techbullion['Title'], Techbullion['Timestamp'], Techbullion['Links'], Techbullion['Text'], Techbullion['Wordslength']  = articles[0],articles[1],articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dailyfintech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:23.106848Z",
     "start_time": "2019-07-29T14:21:21.966107Z"
    }
   },
   "outputs": [],
   "source": [
    "res = requests.get('https://dailyfintech.com/?s=cryptocurrency')\n",
    "soup = bs(res.content,'lxml')\n",
    "articles = [[],[],[],[],[],[]]\n",
    "x = soup.find('div',id='posts-wrapper').find_all(class_='entry-title')\n",
    "for rawlink in x:\n",
    "    link = rawlink.find('a').get('href')\n",
    "    if link not in set(oldlinks):\n",
    "        try:\n",
    "            articlesoup = bs(requests.get(link).text, 'html.parser')\n",
    "            articletext = articlesoup.find('div',class_='entry-content').get_text('',strip=True)\n",
    "            wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "            words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "\n",
    "            articles[0].append(articlesoup.find(class_='entry-title').get_text('',strip=True))\n",
    "            articles[1].append(pd.to_datetime(articlesoup.find('time').get('datetime')))\n",
    "            articles[2].append(link)\n",
    "            articles[3].append(articletext)\n",
    "            articles[4].append(wordslength)\n",
    "            articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "Dailyfintech = pd.DataFrame()\n",
    "Dailyfintech['Title'], Dailyfintech['Timestamp'], Dailyfintech['Links'], Dailyfintech['Text'], Dailyfintech['Wordslength']  = articles[0],articles[1],articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakermag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:23.567930Z",
     "start_time": "2019-07-29T14:21:23.113824Z"
    }
   },
   "outputs": [],
   "source": [
    "res = requests.get('https://breakermag.com/')\n",
    "soup = bs(res.content,'lxml')\n",
    "\n",
    "articles = [[],[],[],[],[],[]]\n",
    "x = soup.find('main').find_all('h2')\n",
    "for rawlink in x:\n",
    "    link = rawlink.find('a').get('href')\n",
    "    if link not in set(oldlinks):        \n",
    "        try:\n",
    "            articlesoup = bs(requests.get(link).text, 'html.parser')\n",
    "           \n",
    "            articletext = articlesoup.find('article').get_text('',strip=True)\n",
    "            wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "            words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "            if articlesoup.find(class_='article-poster__title') is not None:\n",
    "                articles[0].append(articlesoup.find(class_='article-poster__title').get_text('',strip=True)) #Title\n",
    "            else:\n",
    "                articles[0].append(articlesoup.find(class_='article-poster-vertical__headline').get_text('',strip=True))\n",
    "            articles[1].append(pd.to_datetime(articlesoup.find('div',class_='article-poster__posted-date').get_text('',strip=True))) # Time\n",
    "            articles[2].append(link)#Link\n",
    "            articles[3].append(articletext)#Text\n",
    "            articles[4].append(wordslength)\n",
    "            articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "Breakermag = pd.DataFrame()\n",
    "Breakermag['Title'], Breakermag['Timestamp'], Breakermag['Links'], Breakermag['Text'], Breakermag['Wordslength']  = articles[0],articles[1],articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-24T10:41:14.554881Z",
     "start_time": "2019-07-24T10:41:14.547910Z"
    }
   },
   "source": [
    "# Venturebeat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:24.785370Z",
     "start_time": "2019-07-29T14:21:23.570861Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = [[],[],[],[],[],[]]\n",
    "\n",
    "res = requests.get('https://venturebeat.com/?s=cryptocurrency')\n",
    "soup = bs(res.content,'lxml')\n",
    "x = soup.find_all('article')\n",
    "for rawlink in x:\n",
    "        link = rawlink.find('a').get('href')\n",
    "        if link not in set(oldlinks):   \n",
    "            try:\n",
    "                articlesoup = bs(requests.get(link).text, 'html.parser')\n",
    "                \n",
    "                articletext = articlesoup.find('div',id='content').get_text('',strip=True)\n",
    "                wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "                words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "\n",
    "                articles[0].append(articlesoup.find(class_='article-title').get_text('',strip=True))\n",
    "                articles[1].append(pd.to_datetime(articlesoup.find('time').get('datetime'))) # Time\n",
    "                articles[2].append(link)#Link\n",
    "                articles[3].append(articletext)#Text\n",
    "                articles[4].append(wordslength)\n",
    "                articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "                \n",
    "Venturebeat = pd.DataFrame()\n",
    "Venturebeat['Title'], Venturebeat['Timestamp'], Venturebeat['Links'], Venturebeat['Text'], Venturebeat['Wordslength']  = articles[0],articles[1],articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quartz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:25.045354Z",
     "start_time": "2019-07-29T14:21:24.788387Z"
    }
   },
   "outputs": [],
   "source": [
    "articles = [[],[],[],[],[],[]]\n",
    "\n",
    "url = \"https://qz.com/search/cryptocurrency/\"\n",
    "\n",
    "response = requests.get(url, headers)\n",
    "soup = bs(response.text, \"lxml\")\n",
    "x = soup.find(class_='_66ee0 ac70a').find_all('a')\n",
    "for rawlink in x:\n",
    "        link = 'https://qz.com' + rawlink.get('href')\n",
    "        if link not in set(oldlinks):\n",
    "            try:\n",
    "                articlesoup = bs(requests.get(link).text, 'html.parser')\n",
    "                \n",
    "                articletext = articlesoup.find(class_='f46e3').get_text()        \n",
    "                wordslength = len(re.sub('[^a-z\\ \\']+', ' ',  articletext).split())\n",
    "                words = re.sub('[^a-z\\ \\']+', ' ',  articletext).split()\n",
    "                articles[0].append(articlesoup.find('title').get_text())\n",
    "\n",
    "                articles[1].append(pd.to_datetime(articlesoup.find('time').get('datetime')))\n",
    "                articles[2].append(link)\n",
    "                articles[3].append(articletext)\n",
    "                articles[4].append(wordslength)\n",
    "                articles[5].append(words) ### Use to collect all words splitted already for NLP\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "qz = pd.DataFrame()\n",
    "qz['Title'], qz['Timestamp'], qz['Links'], qz['Text'], qz['Wordslength']  = articles[0],pd.to_datetime(articles[1]),articles[2],articles[3],articles[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:25.059324Z",
     "start_time": "2019-07-29T14:21:25.049351Z"
    }
   },
   "outputs": [],
   "source": [
    "def createdf():\n",
    "    df = livecoinwatch.append([coindesk,forexlive,bbc,cointelegraph,Techbullion,Venturebeat,Dailyfintech,Breakermag,qz,Coinspeaker,cryptocompare],ignore_index = True) \n",
    "    df[\"Timestamp\"] = df[\"Timestamp\"].values.astype('datetime64[s]')\n",
    "    df = df.drop_duplicates(subset = 'Title',keep = 'first')\n",
    "    df = df.sort_values(by=\"Timestamp\", ascending = False).reset_index(drop=True)\n",
    "    df = df[[\"Title\",\"Timestamp\",\"Links\",\"Text\",\"Wordslength\"]]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-29T14:21:25.122290Z",
     "start_time": "2019-07-29T14:21:25.064324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = createdf()\n",
    "dataset.to_json('newscrapednews.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
